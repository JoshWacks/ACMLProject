{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> ADAPTIVE COMPUTATION AND MACHINE LEARNING (COMS4030A)\n",
    "\n",
    "## <center> Project: Customer Segmentation \n",
    "<center> Joshua Wacks - 2143116  <center> Alex Vogt - 2152320 <center> Sonia Bullah - 2107762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This research will implement data mining techniques by dividing customers up into various groups based on common features and characteristics, which in turn, could help offer marketing strategies that can improve the relationship between the company and its customers. This analysis will be performed by making use of the K-Means algorithm, which is an unsupervised learning problem, as well as the RFM Segmentation Model. This notebook contains the code used in our implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following imported libraries will be used in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prettytable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcluster\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprettytable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PrettyTable\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmpl_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmplot3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Axes3D\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prettytable'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import missingno\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import datetime\n",
    "import sklearn.cluster as cluster\n",
    "from prettytable import PrettyTable\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Preview Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this research is that of an online retail company that successfully captured online sales during the 2009-2010 period and comprises of 525 461 entries. It contains very useful information that could be used to cluster customers and help evaluate the relationships between certain attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to import as well as preview the data provided by the dataset. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to read in the dataset:\n",
    "df = pd.read_csv(\"online_retail_II.csv\")\n",
    "df = df.rename(columns={\"Customer ID\":\"CustomerID\"})\n",
    "\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])#convert to datetime\n",
    "\n",
    "# Show the first 5 entries of the dataset:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the last 5 entries of the dataset:\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain information on each attribute of the dataset:\n",
    "df.info()\n",
    "\n",
    "#TODO show the range of different fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the descriptive statistics of the dataset:\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all the 0 and negative values with nan for two reasons\n",
    "#1) To visualise the 'irrelevant' entries\n",
    "#2) To treat all these nan values uniformly and drop them\n",
    "df = df.replace(0,np.nan)\n",
    "\n",
    "# Negative quantities refer to returns and that is beyond the scope of this customer segmentation\n",
    "df['Price'][df['Price'] <0] = np.nan  \n",
    "df['Quantity'][df['Quantity'] <0] = np.nan\n",
    "\n",
    "\n",
    "missingno.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, there are some missing values in the dataset. Let's find out which columns contain these null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantity Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if there are any missing or 0 values in the dataset:\n",
    "numNans = df['Quantity'].isna().sum()/len(df) *100\n",
    "print(F\"Number of nan values in the Quantity column {numNans}%. Thus we can drop these rows\")\n",
    "df = df[df['Quantity'].notna()]  # We only take the Quantity values that are not nan\n",
    "#TODO put in report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if there are any missing or 0 values in the dataset:\n",
    "numNans = df['Price'].isna().sum()/len(df) *100\n",
    "print(F\"Number of nan values in the Price column {numNans}%. Thus we can drop these rows\")\n",
    "df = df[df['Price'].notna()]  # We only take the Price values that are not nan\n",
    "#TODO put in report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer ID Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numNans = df['CustomerID'].isna().sum()/len(df) *100\n",
    "print(F\"The number of missing Customer IDs is only {numNans}%. Thus we can drop these rows\")\n",
    "df = df[df['CustomerID'].notna()]  # We only take the Customer ID values that are not nan\n",
    "\n",
    "#TODO discuss why we can do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingno.matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the missing and irrelevant data has been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the descriptive data to numerical values, by means of label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding for non numeric features\n",
    "df['Country'].value_counts()\n",
    "df['Country'] = df['Country'].astype('category').cat.codes\n",
    "df['Invoice'] = df['Invoice'].astype('category').cat.codes\n",
    "df['StockCode'] = df['StockCode'].astype('category').cat.codes\n",
    "df['Description'] = df['Description'].astype('category').cat.codes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the following features:\n",
    "1. Invoice\n",
    "2. StockCode\n",
    "3. Quantity\n",
    "4. Price\n",
    "5. InvoiceDate\n",
    "6. CustomerID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Invoice','StockCode','Quantity','Price','InvoiceDate','CustomerID','Country']] # We drop description\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out any of the none UK transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['Country'] == 34] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an aggregated variable named Amount, by multiplying Quantity with Price, which gives the total amount of money spent per product / item in each transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Amount'] = df['Quantity'] * df['Price'] # Amount = Quantity * Price\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#InvoiceDATE\n",
    "#Season\n",
    "#Time of day\n",
    "#TODO different stock items\n",
    "#Only take popular stock codes\n",
    "\n",
    "#Country and popular stock items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the variable InvoiceDate into two variables Date and Time. This allows different transactions created by the same consumer on the same day but at different times to be treated separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df['InvoiceDate'].dt.date\n",
    "df['Time'] = df['InvoiceDate'].dt.time\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a season column for each transaction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_winter(month_num):\n",
    "\tif ((month_num == 12) | (month_num == 1) | (month_num == 2)):\n",
    "\t\treturn  1 #\"Winter\"\n",
    "\treturn 0\n",
    "\n",
    "def season_spring(month_num):\n",
    "\tif ((month_num == 3) | (month_num == 4) | (month_num == 5)):\n",
    "\t\treturn  1 #\"Spring\"\n",
    "\treturn 0\n",
    "\n",
    "def season_summer(month_num):\n",
    "\tif ((month_num == 6) | (month_num == 7) | (month_num == 8)):\n",
    "\t\treturn  1 #\"Summer\"\n",
    "\treturn 0\n",
    "\n",
    "def season_autumn(month_num):\n",
    "\tif ((month_num == 9) | (month_num == 10) | (month_num == 11)):\n",
    "\t\treturn  1 #\"Autumn\"\n",
    "\treturn 0\n",
    "\n",
    "df['Season0'] = df.apply(lambda x: season_winter(x['Date'].month),axis=1)\n",
    "df['Season1'] = df.apply(lambda x: season_spring(x['Date'].month),axis=1)\n",
    "df['Season2'] = df.apply(lambda x: season_summer(x['Date'].month),axis=1)\n",
    "df['Season3'] = df.apply(lambda x: season_autumn(x['Date'].month),axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 4 new columns: Recency, Frequency, Total_Amount, Season_Total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a season colums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_arr = [\"Season0\",\"Season1\",\"Season2\",\"Season3\"]\n",
    "season_arr_total = [\"Season0Total\",\"Season1Total\",\"Season2Total\",\"Season3Total\"]\n",
    "\n",
    "\n",
    "for sn,head in zip(season_arr,season_arr_total):\n",
    "    df[head] = df.groupby('CustomerID')[sn].transform('sum')    \n",
    "\n",
    "df.drop(columns=season_arr,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a time of day integer column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minutes(t):\n",
    "    return (t.hour * 60 + t.minute) + t.second % 60\n",
    "\n",
    "df['Minutes'] = df.apply(lambda x: get_minutes(x['Time']),axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minutes = df.drop_duplicates('Invoice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total_Amount Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total_Amount'] = df.groupby('CustomerID')['Amount'].transform('sum')\n",
    "df['Min'] = df.groupby('CustomerID')['Amount'].transform('min')\n",
    "df['Max'] = df.groupby('CustomerID')['Amount'].transform('max')\n",
    "df['Avg'] = df.groupby('CustomerID')['Amount'].transform('mean')\n",
    "df['avg_time_minutes'] = df.groupby('CustomerID')['Minutes'].transform('mean')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recency Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_date = datetime.date(2011,1,1)\n",
    "df.sort_values(by='InvoiceDate',ascending=False).groupby('CustomerID')\n",
    "df['Recency'] = (simulated_date.year - pd.DatetimeIndex(df['Date']).year) * 12  + (simulated_date.month - pd.DatetimeIndex(df['Date']).month)\n",
    "df['Recency'] = df['Recency'] \n",
    "\n",
    "res = df.groupby(['CustomerID']).apply(lambda x:x['Recency'].min())  # Get the recency for each customer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = df.groupby(['CustomerID','Invoice']).size().reset_index(drop=False).groupby('CustomerID')[[0]].count()  # Get the frequency for each customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condensing table contents per user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon =  df.groupby(['CustomerID'])['Total_Amount'].unique().astype(float)\n",
    "min =  df.groupby(['CustomerID'])['Min'].unique().astype(float)\n",
    "max =  df.groupby(['CustomerID'])['Max'].unique().astype(float)\n",
    "mean =  df.groupby(['CustomerID'])['Avg'].unique().astype(float)\n",
    "\n",
    "season0 = df.groupby(['CustomerID'])['Season0Total'].unique().astype(int)\n",
    "season1 = df.groupby(['CustomerID'])['Season1Total'].unique().astype(int)\n",
    "season2 = df.groupby(['CustomerID'])['Season2Total'].unique().astype(int)\n",
    "season3 = df.groupby(['CustomerID'])['Season3Total'].unique().astype(int)\n",
    "\n",
    "avg_minutes = df.groupby(['CustomerID'])['avg_time_minutes'].unique().astype(float)\n",
    "\n",
    "df_customer = pd.DataFrame({'Recency':res,'Frequency':freq[0],'Total_Spent':mon,\n",
    "                       'Min_Spent':min,'Max_Spent':max,'Mean_Spent':mean,\n",
    "                       'Season0':season0,'Season1':season1,'Season2':season2,'Season3':season3,\n",
    "                       'Avg_Time_Minutes': avg_minutes})\n",
    "\n",
    "df_customer['Popular_Season'] = df_customer[['Season0','Season1','Season2','Season3']].idxmax(axis =1) # Get the most popular season for that customer\n",
    "df_customer.head() # This df_customer has info pertaining to each individual customer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer['Popular_Season'] = df_customer['Popular_Season'].astype('category').cat.codes\n",
    "df_customer.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_subplots():\n",
    "\tfor clm in ['Recency','Frequency']:\n",
    "\t\tplt.figure(figsize=(15,7))\n",
    "\t\tsns.countplot(data = df_customer, x = clm)\n",
    "\t\tplt.show()\n",
    "\t\n",
    "\t\tsns.violinplot(data = df_customer, y = clm)\n",
    "\t\tplt.show()\n",
    "\t\n",
    "\tplt.figure(figsize=(15,7))\t\n",
    " \n",
    "\tsns.scatterplot(data = df_customer.Total_Spent)\n",
    "\tplt.show()\n",
    "outlier_subplots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers():\n",
    "\tcondition = df_customer.index[df_customer['Frequency'] > 30]\n",
    "\tdf_customer.drop(condition, inplace=True)\n",
    " \n",
    "\tcondition = df_customer.index[df_customer['Total_Spent'] > 20000]\n",
    "\tdf_customer.drop(condition, inplace=True)\n",
    "\n",
    " \n",
    "remove_outliers()\n",
    "outlier_subplots()\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.histplot(data = df_customer['Total_Spent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_std():\n",
    "\n",
    "\tdf_new['Recency']=(df_new['Recency']-df_new['Recency'].mean())/df_new['Recency'].std()\n",
    "\tdf_new['Frequency']=(df_new['Frequency']-df_new['Frequency'].mean())/df_new['Frequency'].std()\n",
    "\tdf_new['Total_Amount']=(df_new['Total_Amount']-df_new['Total_Amount'].mean())/df_new['Total_Amount'].std()\n",
    " \n",
    "\tfor sn in season_arr:\n",
    "\t\tdf_new[sn] = (df_new[sn]-df_new[sn].mean())/df_new[sn].std()\n",
    "# normalise_std()\n",
    "# df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_old = df_customer.copy() #Need to make a copy to store old max and min values\n",
    "def normalise_min_max():\n",
    "\tfor clm in df_customer.columns:\n",
    "\t\tX_scaled = (df_customer[clm] - df_customer[clm].min(axis=0)) / (df_customer[clm].max(axis=0) - df_customer[clm].min(axis=0))\n",
    "\t\tdf_customer[clm] = X_scaled\n",
    "normalise_min_max()\n",
    "\n",
    "\n",
    "df_customer.head()\n",
    "df_customer_old.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undo_normalising(x,xMin,xMax):\n",
    "    return x*(xMax - xMin) + xMin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "def three_d_scatter(df_clusters,num_clusters,labels):\n",
    "\tsns.set(style = \"darkgrid\")\n",
    "\tfig = plt.figure(figsize=(15,7))\n",
    "\t# ax = fig.add_subplot(111, projection = '3d')\n",
    "\tax = Axes3D(fig)\n",
    "\tif num_clusters == 0:\n",
    "\t\tnum_clusters = df_clusters['cluster'].max() - df_clusters['cluster'].min()\n",
    "\t\n",
    "\tprint(num_clusters)\n",
    "\tax.set_xlabel(labels[0])\n",
    "\tax.set_ylabel(labels[1])\n",
    "\tax.set_zlabel(labels[2])\n",
    " \n",
    "\tcolor_arr = ['blue','red','green','black','orange','purple','yellow']\n",
    "\tshape_arr = ['.','x','^','*','o','s','d']\n",
    "\t\n",
    "\n",
    "\tfor i in range(num_clusters):\n",
    "\t\tx = df_clusters[labels[0]].where(df_clusters['cluster'] == i)\n",
    "\t\ty = df_clusters[labels[1]].where(df_clusters['cluster'] == i)\n",
    "\t\tz = df_clusters[labels[2]].where(df_clusters['cluster'] == i)\n",
    "\n",
    "\t\tax.scatter( x, y, z,color = color_arr[i], s = 20,marker = shape_arr[i],label = F\" Cluster {i}\")\n",
    "\t\n",
    "\tplt.legend()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_analysis(df_clusters,num_centres,labels,num_dim):\n",
    "\ttable =  PrettyTable()\n",
    "\ttable.field_names = [\"Cluster\",\"Mean\",\"Max\",\"Min\"]\n",
    "\tclmns = {}\n",
    "\tfor i in range(num_centres):\n",
    "\t\tcolumn1 = df_clusters[labels[0]].loc[df_clusters['cluster'] == i]\n",
    "\t\tlabel1 = labels[0] + str(i)\n",
    "  \n",
    "\t\tcolumn2 = df_clusters[labels[1]].loc[df_clusters['cluster'] == i]\n",
    "\t\tlabel2 = labels[1] + str(i)\n",
    "  \n",
    "\t\tclmns[label1] = column1\n",
    "\t\tclmns[label2] = column2\n",
    "  \n",
    "\t\tif num_dim == 3:\n",
    "\t\t\tcolumn3 = df_clusters[labels[2]].loc[df_clusters['cluster'] == i]\n",
    "\t\t\tlabel3 = labels[2] + str(i)\n",
    "\t\t\tclmns[label3] = column3\n",
    "\n",
    "\t\t# table.add_row([F\"Cluster {i}:\",'','',''])\n",
    "\n",
    "\n",
    "\tfor i,clm in enumerate(clmns):\n",
    "\t\tif i % num_dim  == 0:\n",
    "\t\t\ttable.add_row([F\"Cluster {clm[-1]}:\",'','',''])\n",
    "\t\t\t\n",
    "\t\tlabel = clm[:-1]\n",
    "\t\tclm = clmns[clm]\n",
    "\t\tcluster_mean = clm.mean()\n",
    "\t\tcluster_max = clm.max()\n",
    "\t\tcluster_min = clm.min()\n",
    "\t\toriginal_mean = undo_normalising(cluster_mean,df_customer_old[label].min(axis=0),df_customer_old[label].max(axis=0))\n",
    "\t\toriginal_max = undo_normalising(cluster_max,df_customer_old[label].min(axis=0),df_customer_old[label].max(axis=0))\n",
    "\t\toriginal_min = undo_normalising(cluster_min,df_customer_old[label].min(axis=0),df_customer_old[label].max(axis=0))\n",
    "\t\toriginal_mean = np.round(original_mean,2)\n",
    "\t\toriginal_max = np.round(original_max,2)\n",
    "\t\toriginal_min = np.round(original_min,2)\n",
    "\n",
    "\t\ttable.add_row([label,original_mean,original_max,original_min])\n",
    "\t\t# table.add_row([label,cluster_mean,cluster_max,cluster_min])\n",
    "  \n",
    "  \n",
    "\tprint(table)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_2_Features():\n",
    "\t%matplotlib inline\n",
    "\tnum_centres = [3,3,5,3,3]\n",
    "\tcomparison_arr =[['Total_Spent','Frequency'],['Recency','Frequency'],['Avg_Time_Minutes','Frequency'],['Avg_Time_Minutes','Total_Spent']]\n",
    "\tfor nc,comp in zip(num_centres,comparison_arr):\n",
    "     \n",
    "\t\tnp_array = df_customer[comp].to_numpy()\n",
    "\t\tkmeans = cluster.KMeans(n_clusters=nc,random_state=42).fit(np_array)\n",
    "\t\tdf_clusters = df_customer.copy()\n",
    "\t\tdf_clusters['cluster'] = kmeans.labels_\n",
    " \n",
    "\t\tplt.figure(figsize=(15,7))\n",
    "\t\tsns.scatterplot( x = df_clusters[comp[0]],y = df_clusters[comp[1]], hue = df_clusters['cluster'],palette=\"bright\")\n",
    "\t\tplt.title(F\"{comp[1]} VS {comp[0]}\")\n",
    "\t\tplt.show()\n",
    "\t\tcluster_analysis(df_clusters,nc,comp,2)\n",
    "k_means_2_Features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_3_Features():\n",
    "\t%matplotlib widget\n",
    "\tnum_centres = [3,4,3]\n",
    "\tcomparison_arr = [['Total_Spent','Frequency','Recency'],['Popular_Season','Total_Spent','Frequency'],['Total_Spent','Frequency','Avg_Time_Minutes']]\n",
    "\n",
    "\tfor nc,comp in zip(num_centres,comparison_arr):\n",
    "\t\tnp_array = df_customer[comp].to_numpy()\n",
    "\t\tkmeans = cluster.KMeans(n_clusters=nc,random_state=42).fit(np_array)\n",
    "\t\tdf_clusters = df_customer.copy()\n",
    "\t\tdf_clusters['cluster'] = kmeans.labels_\n",
    "\t\tthree_d_scatter(df_clusters,nc,comp)\n",
    "\t\tcluster_analysis(df_clusters,nc,comp,3)\n",
    "  \n",
    "k_means_3_Features()\n",
    "\n",
    "#TODO plot individual clusters"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01b0da322a7df2b881bf69dce4c75684d5ac75b853286a49a713693279c2c23c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
